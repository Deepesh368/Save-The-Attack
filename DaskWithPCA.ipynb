{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# !pip install dask-ml[complete]\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-23T15:49:29.423818Z","iopub.execute_input":"2021-12-23T15:49:29.424093Z","iopub.status.idle":"2021-12-23T15:49:29.441251Z","shell.execute_reply.started":"2021-12-23T15:49:29.424022Z","shell.execute_reply":"2021-12-23T15:49:29.440346Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import dask.dataframe as dd\nimport seaborn as sns \nimport dask_ml\nimport matplotlib.pyplot as plt\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-12-23T15:49:29.442759Z","iopub.execute_input":"2021-12-23T15:49:29.443121Z","iopub.status.idle":"2021-12-23T15:49:31.125064Z","shell.execute_reply.started":"2021-12-23T15:49:29.443090Z","shell.execute_reply":"2021-12-23T15:49:31.124314Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport plotly.express as px\nimport dask.array as da\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-12-23T15:49:31.126290Z","iopub.execute_input":"2021-12-23T15:49:31.126503Z","iopub.status.idle":"2021-12-23T15:49:31.766323Z","shell.execute_reply.started":"2021-12-23T15:49:31.126477Z","shell.execute_reply":"2021-12-23T15:49:31.765495Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"columns = list(dd.read_csv(\"/kaggle/input/nanoutlierremoveddataset/NaNOutlierRemovedDataset.csv\",dtype={'Census_ProcessorClass':'object','PuaMode':'object'}).head())\ntrain_dataset = dd.read_csv(\"/kaggle/input/nanoutlierremoveddataset/NaNOutlierRemovedDataset.csv\",dtype={'Census_ProcessorClass':'object','PuaMode':'object'},usecols = [i for i in columns if i != 'PuaMode'])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T15:49:31.768116Z","iopub.execute_input":"2021-12-23T15:49:31.768388Z","iopub.status.idle":"2021-12-23T15:49:34.815553Z","shell.execute_reply.started":"2021-12-23T15:49:31.768358Z","shell.execute_reply":"2021-12-23T15:49:34.814947Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Next cell is to show the distribution of target values in the columns to label encode to show the ordering.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nanoutlierremoveddataset/NaNOutlierRemovedDataset.csv\",usecols = columns)\nrows = train_df.shape[0]\nlabel_columns = ['ProductName','EngineVersion','AppVersion','AvSigVersion','AVProductsInstalled','AVProductsEnabled']\n\nd_list =[]\nfor c in label_columns:\n#     print(train_df[c].unique())\n    values = list(train_df[c].unique())\n#     print(values)\n    yes,no,labels = [],[],[]\n    total = 0\n    dic = {}\n    for val in values:\n#         print(train_dataset[da.logical_and(train_dataset[c]==values[key],train_dataset['HasDetections']==1)].compute().shape[0])\n#         y = int(train_df[(train_df[c]==val and train_df['HasDetections']==1)].shape[0])\n        temp = train_df[train_df[c]==val]\n        y = int(temp[temp['HasDetections']==1].shape[0])\n        n = int(temp[temp['HasDetections']==0].shape[0])\n        del temp\n        gc.collect()\n        yes.append(y)\n        no.append(n)\n        dic[val]=y+n\n        labels.append(val)\n    #sort dic\n    d = dict(sorted(dic.items(), key = lambda kv: kv[1],reverse=True))\n    print(d)\n    d_list.append(d)\n    fig,ax = plt.subplots()\n    ax.bar(labels,yes,0.35,label = 'Yes')\n    ax.bar(labels,no,0.35,label = 'No')\n    ax.set_ylabel(\"Total number of values\")\n    ax.set_title(\"Distribution of output values in \"+c)\n    ax.legend()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T15:49:34.817460Z","iopub.execute_input":"2021-12-23T15:49:34.817969Z","iopub.status.idle":"2021-12-23T15:49:42.867865Z","shell.execute_reply.started":"2021-12-23T15:49:34.817909Z","shell.execute_reply":"2021-12-23T15:49:42.866577Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"columns_to_drop = ['AvSigVersion','Census_OSVersion','OsBuildLab']\nfor c in columns_to_drop:\n    train_dataset = train_dataset.drop(c,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_get_from_test = train_dataset.columns.tolist()\ncolumns_to_get_from_test.remove('HasDetections')\ncategorical_columns = []\nfor c in train_dataset.columns:\n    if train_dataset[c].dtype == 'object':\n        categorical_columns.append(c)\n\ntrain_dataset = train_dataset.drop(columns ='MachineIdentifier')\ntrain_dataset = train_dataset.categorize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_columns = train_dataset.columns.tolist()\nfor c in categorical_columns:\n    if c in continuous_columns:\n        continuous_columns.remove(c)\ncategorical_columns.remove('MachineIdentifier')\ntrain_dataset = dd.get_dummies(train_dataset,columns = categorical_columns)\n\ny_train = train_dataset['HasDetections']\ntrain_dataset = train_dataset.drop(columns='HasDetections')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = pd.read_csv(\"../input/save-the-attack-contest/test_data.csv\", usecols = [i for i in columns_to_get_from_test])\n\ntest_machine_id = test_dataset['MachineIdentifier']\ntest_dataset = test_dataset.drop(columns = 'MachineIdentifier')\n\nfor column in categorical_columns:\n    test_dataset[column].fillna(test_dataset[column].value_counts().idxmax(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in continuous_columns:\n    if(column == 'HasDetections'):\n        continue\n    test_dataset[column].fillna(test_dataset[column].mean(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.to_csv('test_dataset_filled.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = dd.read_csv(\"./test_dataset_filled.csv\")\n\n#one hot encoding\ncategorical_columns = test_dataset.select_dtypes(include=['object']).columns.tolist()\ntest_dataset = test_dataset.categorize()\ntest_dataset = dd.get_dummies(test_dataset,columns = categorical_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_columns = train_dataset.columns.tolist()\ntest_columns = test_dataset.columns.tolist()\n\nfor column in train_columns:\n    if column not in test_columns:\n        train_dataset = train_dataset.drop(columns = column)\n\ntrain_columns = train_dataset.columns.tolist()\ntest_columns = test_dataset.columns.tolist()\n\nfor column in test_columns:\n    if column not in train_columns:\n        test_dataset = test_dataset.drop(columns = column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset[sorted(train_dataset.columns)]\ntest_dataset = test_dataset[sorted(test_dataset.columns)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dask_ml.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ntrain_dataset = scaler.fit_transform(train_dataset)\ntest_dataset = scaler.transform(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PCA\nfrom dask_ml.decomposition import IncrementalPCA\n\npca = IncrementalPCA(n_components = 7,copy=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.fit(train_dataset.to_dask_array(lengths=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.transform(test_dataset.to_dask_array(lengths=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset.to_csv(\"trainDatasetFinal.csv\",index=False,single_file=True)\n# test_dataset.to_csv(\"test_dataset_final.csv\",index=False,single_file=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sgdClassifier\nfrom dask_ml.datasets import make_classification\n\nfrom dask_ml.wrappers import Incremental\n\nfrom sklearn.linear_model import SGDClassifier\n\nestimator = SGDClassifier(loss='log', max_iter=1000)\n\nclf = Incremental(estimator)\n\nclf.fit(train_dataset, y_train, classes = [0, 1])\n\nimport dask.bag as db\n\nypred = clf.predict(test_dataset)\nypred = ypred.compute()\nprediction_df = pd.DataFrame({'MachineIdentifier':test_machine_id,'HasDetections':ypred})\nprediction_df.to_csv('test_data_predictions1.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MLP classifier\nfrom dask_ml.datasets import make_classification\n\nfrom dask_ml.wrappers import Incremental\n\nfrom sklearn.neural_network import MLPClassifier\n\nestimator = MLPClassifier(activation='logistic', max_iter=3000)\n\nclf = Incremental(estimator)\n\nclf.fit(train_dataset, y_train, classes = [0, 1])\n\nimport dask.bag as db\n\nypred = clf.predict(test_dataset)\nypred = ypred.compute()\n\nprediction_df = pd.DataFrame({'MachineIdentifier':test_machine_id,'HasDetections':ypred})\nprediction_df.to_csv('test_data_predictions1.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adaboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}